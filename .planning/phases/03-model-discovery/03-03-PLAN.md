---
phase: 03-model-discovery
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - app/(dashboard)/training/components/ai-recommender.tsx
  - app/(dashboard)/training/actions.ts
  - app/(dashboard)/training/page.tsx
autonomous: true
user_setup:
  - service: huggingface
    why: "AI model recommendations via Inference API require authentication"
    env_vars:
      - name: HUGGINGFACE_TOKEN
        source: "HuggingFace -> Settings -> Access Tokens -> New token (read permission)"
    dashboard_config: []

must_haves:
  truths:
    - "User can describe a training goal in natural language and receive model suggestions"
    - "Recommendations show which Liquid LFM best fits the described goal with reasoning"
    - "Recommender works gracefully if Inference API is unavailable (rule-based fallback)"
  artifacts:
    - path: "app/(dashboard)/training/components/ai-recommender.tsx"
      provides: "Natural language input with AI-powered model recommendations"
      min_lines: 40
    - path: "app/(dashboard)/training/actions.ts"
      provides: "recommendModel Server Action added alongside searchModels"
      exports: ["searchModels", "recommendModel"]
  key_links:
    - from: "app/(dashboard)/training/components/ai-recommender.tsx"
      to: "app/(dashboard)/training/actions.ts"
      via: "Calls recommendModel Server Action on submit"
      pattern: "recommendModel"
    - from: "app/(dashboard)/training/actions.ts"
      to: "lib/huggingface/liquid-lfm.ts"
      via: "Uses LIQUID_LFMS in prompt context and as rule-based fallback"
      pattern: "LIQUID_LFMS"
---

<objective>
Add natural language AI recommender to the training page — users describe their goal and get Liquid LFM suggestions.

Purpose: This is the "wow" feature for model discovery. Instead of browsing, users describe what they want to accomplish and get intelligent suggestions. Demonstrates accessible ML UX.
Output: AI recommender section on training page with LLM-backed suggestions and rule-based fallback.
</objective>

<execution_context>
@/home/wraith/.claude/get-shit-done/workflows/execute-plan.md
@/home/wraith/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-model-discovery/03-RESEARCH.md
@.planning/phases/03-model-discovery/03-01-SUMMARY.md
@lib/huggingface/types.ts
@lib/huggingface/liquid-lfm.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: AI recommender Server Action with LLM + rule-based fallback</name>
  <files>app/(dashboard)/training/actions.ts</files>
  <action>
Install: `pnpm add @huggingface/inference`

Add `recommendModel` Server Action to the existing `actions.ts` file (do NOT overwrite searchModels):

- Validate input with zod: string, min 5 chars, max 500 chars
- **Primary path (LLM):** If `HUGGINGFACE_TOKEN` env var exists:
  - Create `InferenceClient` with token
  - Call `client.chatCompletion()` using a free model like `meta-llama/Llama-3.1-8B-Instruct`
  - Prompt includes the full Liquid LFM catalog (from LIQUID_LFMS) with names, modalities, param counts, descriptions
  - Ask the model to recommend 1-2 best fits with reasoning, formatted as JSON: `{ recommendations: [{ modelId: string, reason: string }] }`
  - Parse LLM response, match modelId back to LIQUID_LFMS entries
  - Return `{ success: true, recommendations: ModelInfo[], reasoning: string }`

- **Fallback path (rule-based):** If no token or LLM call fails:
  - Simple keyword matching on user goal:
    - "image", "photo", "visual", "see", "picture" -> vision models
    - "audio", "sound", "speech", "voice", "listen" -> audio models
    - "edge", "mobile", "small", "fast", "lite", "embedded" -> nano models
    - Default -> text models
  - Return matched models with generic reasoning: "Based on your goal mentioning [keyword], we recommend these [modality] models."
  - Return `{ success: true, recommendations: ModelInfo[], reasoning: string, fallback: true }`

- On any error: return `{ success: false, error: "Unable to generate recommendations. Please try again." }`
  </action>
  <verify>
`pnpm build` succeeds. Both `searchModels` and `recommendModel` are exported from actions.ts.
  </verify>
  <done>recommendModel Server Action tries LLM inference first, falls back to keyword matching, never crashes.</done>
</task>

<task type="auto">
  <name>Task 2: AI recommender UI component and page integration</name>
  <files>app/(dashboard)/training/components/ai-recommender.tsx, app/(dashboard)/training/page.tsx</files>
  <action>
Create `app/(dashboard)/training/components/ai-recommender.tsx` ("use client"):
- Render a visually distinct section (maybe a Card with a subtle gradient border or accent background using Liquid AI purple)
- Header: sparkles icon (lucide Sparkles) + "AI Model Advisor" title + subtitle "Describe your training goal and get personalized recommendations"
- ShadCN Textarea for input (not Input — goals can be longer), placeholder "e.g., I want to build a chatbot that runs on mobile devices..."
- Submit button with Sparkles icon, disabled when input < 5 chars or isPending
- Use `useTransition` for pending state. On submit: call `recommendModel(goal)` Server Action
- **Results display:**
  - Show reasoning text in a muted callout/alert
  - Show recommended ModelCard(s) below reasoning
  - If `fallback: true` in response, show a subtle note: "AI advisor unavailable — showing keyword-based suggestions"
- **States:** idle (just input), pending (spinner + "Thinking..."), results (reasoning + cards), error (friendly message + retry button)
- Include "Try another" button in results state to clear and start over

Update `app/(dashboard)/training/page.tsx`:
- Replace the placeholder #ai-recommender div with the AiRecommender client component
- Place it ABOVE the curated section (it's the hero feature)
- Add a ShadCN Separator between recommender and curated sections
  </action>
  <verify>
`pnpm build` succeeds. Visit /training — AI recommender section visible at top. Type a goal, submit — see recommendations (LLM-backed if token set, rule-based otherwise). Loading state shows during inference. Error state recoverable.
  </verify>
  <done>Users describe their training goal in natural language, get model recommendations with reasoning. Works with or without HuggingFace token. Visually polished as hero feature.</done>
</task>

</tasks>

<verification>
- `pnpm build` passes with no errors
- AI recommender visible at top of /training page
- Submitting a goal returns recommendations with reasoning
- Without HUGGINGFACE_TOKEN: rule-based fallback works
- With HUGGINGFACE_TOKEN: LLM-backed recommendations work
- Loading, error, and empty states all handled
- Both searchModels and recommendModel coexist in actions.ts
</verification>

<success_criteria>
User describes "I want to process audio from phone calls" and gets audio model recommendations. User describes "build a chatbot for mobile" and gets nano/text recommendations. Works without API token via keyword fallback. Recommender section is visually prominent and polished.
</success_criteria>

<output>
After completion, create `.planning/phases/03-model-discovery/03-03-SUMMARY.md`
</output>
